---
title: 'MSiA 490-20 / Ling 400: Homework 2'
author: "Instructor: Klinton Bicknell"
output:
  html_document:
    highlight: pygments
---
_parts adapted from Jorge Moraleda_

**Turning it in**: You'll turn in a zip file via Canvas that contains (a) a write-up as a pdf and (b) the other items described below.

## Tokenization and normalization
The high level goal of this part of the assignment is to explore how differences in preprocessing can affect text analytic results. This also serves to introduce [Apache Lucene Core](https://lucene.apache.org/core/), a powerful text indexing and retrieval library widely used in industry, which we will also be using for later assignments.

1. **Tokenization.** You will compare two widely used tokenizers on a short piece of text named `wsj_0063`. This text was originally published in the _Wall Street Journal_ in 1989 and is now part of the [Penn Treebank](https://www.cis.upenn.edu/~treebank/), a widely used corpus that has been annotated for grammatical structure. While the full Penn Treebank is not free, this file can be found in the corpus's [free sample](http://www.nltk.org/nltk_data/packages/corpora/treebank.zip) as included in the [NLTK](www.nltk.org) project. Note that this data is licensed for non-commercial use only. For the purposes of this exercise we'll be using the raw version of this text, available at `raw/wsj_0063`.

      The two tokenizers to compare are:

      * The [Stanford tokenizer](http://nlp.stanford.edu/software/tokenizer.shtml), included in [Stanford CoreNLP](http://nlp.stanford.edu/software/corenlp.shtml), which was used for the previous assignment. Just as in the previous assignment, note that [Brendan O'Connor's python wrapper](https://github.com/brendano/stanford_corenlp_pywrapper) works well on linux and macs.
      * The standard unicode tokenizer, implemented by [Lucene Core's](https://lucene.apache.org/core/) class [`StandardTokenizer`](http://lucene.apache.org/core/5_3_1/analyzers-common/org/apache/lucene/analysis/standard/StandardTokenizer.html). Like CoreNLP, Lucene is Java software, so if you want to use this on python, you'll need a wrapper. The official one is [PyLucene](http://lucene.apache.org/pylucene/). Note also that, unlike the python wrappers for CoreNLP, PyLucene also works on all major platforms, including Windows.

      You will report the tokenization differences between both approaches. For each line where the tokenization differs between the two approaches, show the original string and the two alternative tokenizations. What patterns do you see in the differences?

2. **Normalization.** You will compare four widely used normalization algorithms, also on `wsj_0063`. Note that in Lucene Core, normalization algorithms are referred to as 'analyzers'. There is a [list of analyzers for English](http://lucene.apache.org/core/5_3_1/analyzers-common/org/apache/lucene/analysis/en/EnglishAnalyzer.html)

      * Lucene Core's [`EnglishAnalyzer`](http://lucene.apache.org/core/5_3_1/analyzers-common/org/apache/lucene/analysis/en/EnglishAnalyzer.html): Lucene Core's default for English, implementing the Porter Stemmer version 2
      * Lucene Core's [`PorterStemFilter`](http://lucene.apache.org/core/5_3_1/analyzers-common/org/apache/lucene/analysis/en/PorterStemFilter.html): implementing the original Porter Stemmer, which is also widely used in analytics
      * Lucene Core's [`KStemFilter`](http://lucene.apache.org/core/5_3_1/analyzers-common/org/apache/lucene/analysis/en/KStemFilter.html): a less aggressive stemmer
      * CoreNLP's lemmatizer: a full-on lemmatizer, not just a quick-and-dirty stemmer; available by requesting `lemma` annotation in CoreNLP.
      
      As for the previous exercise, you will report the differences between these four approaches, for each line where the normalization differs between any of them. Describe the main ways the lemmatizer differs from the stemmers. Also describe the patterns you see in the differences between stemmers.
      
      Note: tokenization must be performed first before normalization. To make this simple, you can use Lucene to tokenize prior to applying the Lucene normalization algorithms and use CoreNLP to tokenizer prior to applying the CoreNLP normalization algorithms.
      
3. **Tokenization and normalization.** You will decide on a tokenization and normalization algorithm to apply to `classbios.txt` from the previous assignment. Make an argument as to why you selected that particular tokenization and normalization algorithm. Include your normalized `classbios.txt` file in the zip.

4. **Basic frequency analysis.** Write a program to calculate the frequency of each (normalized) word in the normalized `classbios.txt` corpus. Report the top 20 most frequent words, excluding stop words and punctuation, and their frequencies. Describe whether you think this list gives much information about this corpus. Did you learn anything about the backgrounds of the class?

5. **Basic bigram analysis.** Write a program to calculate the frequency of each bigram in the corpus (i.e., sequence of two words). Report the top 20 most frequent bigrams, excluding bigrams that include a stop word or punctuation, and their frequencies. Describe whether you think this list gives much information about this corpus. Did you learn anything about the backgrounds of the class?

6. **Sentiment analysis.** TBA
